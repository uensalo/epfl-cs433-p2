import torch
from torch import nn 
import numpy as np
from PIL import Image
from skimage.transform import resize
import skimage
import itertools
import scipy
import matplotlib.pyplot as plt
import os
import sys
import argparse
from tqdm import tqdm

import torchvision.transforms.functional as TF
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import confusion_matrix,precision_score,f1_score,recall_score,accuracy_score
import torchvision.transforms as transforms
import random
import skimage
import requests

import zipfile

from clint.textui import progress

# Create the parser
parser = argparse.ArgumentParser(description='Replicates our submission to AICrowd for EPFL CS-433 Fall 2020, Road Segmentation.')

parser.add_argument('--model_path',
                       metavar='models/model_shatznet_god.model',
                       type=str,
                       help='full path to model file',
                       default='models/model_shatznet_god.model')

parser.add_argument('--train_path',
                       metavar='training/',
                       type=str,
                       help='full path to training set directory, the directory must contain two subdirectories titled groundtruth/ and images/, and the image files should be called satImage_00x.png',
                       default='training/')

parser.add_argument('--test_path',
                       metavar='test_set_images/',
                       type=str,
                       help='full path to test set directory, the directory must contain a subdirectory called test_x for every test image, and each subdirectory must contain a test_x.png',
                       default='test_set_images/')

parser.add_argument('--data_path',
                       metavar='data/',
                       type=str,
                       help='full path to data file directory, the directory must contain the generated data, augmented_images_train_best.npy and augmented_labels_train_best.npy',
                       default='data/')


parser.add_argument('--load_img', dest='load_img', action='store_true',help='load training set in memory')
parser.add_argument('--no-load_img', dest='load_img', action='store_false',help='does not load training set in memory')
parser.set_defaults(load_img=False)

parser.add_argument('--load_data', dest='load_data', action='store_true', help='load generated training images in memory')
parser.add_argument('--no-load_data', dest='load_data', action='store_false', help='does not load generated training images in memory')
parser.set_defaults(load_data=False)

parser.add_argument('--load_model', dest='load_model', action='store_true', help='load pretrained model in memory')
parser.add_argument('--no-load_model', dest='load_model', action='store_false', help='does not load pretrained model in memory')
parser.set_defaults(load_model=True)

parser.add_argument('--generate_data', dest='generate_data', action='store_true', help='generate data and save')
parser.add_argument('--no-generate_data', dest='generate_data', action='store_false', help='does not generate data and save')
parser.set_defaults(generate_data=False)

parser.add_argument('--train', dest='train', action='store_true', help='perform training')
parser.add_argument('--no-train', dest='train', action='store_false', help='does not perform training')
parser.set_defaults(train=False)

parser.add_argument('--cuda', dest='cuda', action='store_true', help='use cuda, true recommended')
parser.add_argument('--no-cuda', dest='cuda',action='store_false', help='does not use cuda, true recommended')
parser.set_defaults(cuda=True)

parser.add_argument('--dmodel', dest='d_model', action='store_true', help='download the latest model and save')
parser.add_argument('--no-dmodel', dest='d_model',action='store_false', help='does not download the latest model and save')
parser.set_defaults(d_model=False)


# Execute the parse_args() method
args = parser.parse_args()
LOAD_IMG = args.load_img
LOAD_DATA  = args.load_data
LOAD_MODEL = args.load_model
GENERATE_DATA = args.generate_data
TRAIN = args.train

MODEL_PATH = args.model_path
TRAIN_PATH = args.train_path
TEST_PATH = args.test_path
DATA_PATH = args.data_path

if args.generate_data and not args.load_img:
    print ("ERROR: Can not generate data without loading images into memory. Make sure to run --generate_data with --load_img.")
    sys.exit(-1)

if args.train and not os.path.exists(DATA_PATH):
    print("ERROR: Training was requested but generated dataset not found. Please supply data in the format generated by this script, under {DATA_PATH}augmented_images_train_best and {DATA_PATH}augmented_labels_train_best. To obtain these, you could also run with --load_img --generate_data")
    sys.exit(-1)

if (args.generate_data or args.load_img) and not os.path.exists(TRAIN_PATH):
    url = "https://github.com/uensalo/epfl-cs433-p2/raw/main/training.zip"
    print ("WARNING: Loading train images into memory or dataset generation requested, but training data not found...")
    print ("Train set downloading...")
    r = requests.get(url, stream=True)

    with open('training.zip', 'wb') as f:
        total_length = int(r.headers.get('content-length'))
        for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length/1024) + 1): 
            if chunk:
                f.write(chunk)
                f.flush()


    with zipfile.ZipFile('training.zip', 'r') as zip_ref:
        zip_ref.extractall()

    os.remove('training.zip')


if not os.path.exists(TEST_PATH):
    url = "https://github.com/uensalo/epfl-cs433-p2/raw/main/test_set_images.zip"
    print ("WARNING: Test set not found...")
    print ("Test set downloading...")
    r = requests.get(url, stream=True)

    with open('test_set_images.zip', 'wb') as f:
        total_length = int(r.headers.get('content-length'))
        for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length/1024) + 1): 
            if chunk:
                f.write(chunk)
                f.flush()


    with zipfile.ZipFile('test_set_images.zip', 'r') as zip_ref:
        zip_ref.extractall()

    os.remove('test_set_images.zip')

if (args.d_model or not os.path.exists(MODEL_PATH)) and not args.train:
    url = "https://github.com/uensalo/epfl-cs433-p2/raw/main/models/model_shatznet_god.model"
    print ("Model download was requested or model not found...")
    print ("Best model (model_shatznet_god.model) downloading...")
    r = requests.get(url, stream=True)
    if not os.path.exists(os.path.split(MODEL_PATH)[0]):
        os.makedirs(os.path.split(MODEL_PATH)[0])

    with open(MODEL_PATH, 'wb') as f:
        total_length = int(r.headers.get('content-length'))
        for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length/1024) + 1): 
            if chunk:
                f.write(chunk)
                f.flush()

if args.generate_data:
    if not os.path.exists('data'):
        os.makedirs('data')

use_cuda = args.cuda 
if not(torch.cuda.is_available()) and use_cuda:
    print('ERROR: CUDA requested but unavailable, quitting...')
    sys.exit(-1)

if torch.cuda.is_available() and not(use_cuda):
    print('WARNING: CUDA not requested but is available and recommended...')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def batch(images,labels,batch_size):
    k=0
    while k < images.shape[0]:
        yield images[k:min(k+batch_size,images.shape[0])], labels[k:min(k+batch_size,labels.shape[0])]
        k+=batch_size

def display_image(img):
    plt.imshow(np.transpose(img,(1,2,0)))

class WNetNode(nn.Module):
    def __init__(self, in_channels, out_channels, last=False, activation=nn.LeakyReLU(), kernel_size=3, padding=1):
        super(WNetNode,self).__init__()
        self.batchnorm1 = nn.BatchNorm2d(out_channels)
        self.batchnorm2 = nn.BatchNorm2d(out_channels)
        self.activation = activation
        self.layer1 = nn.Conv2d(in_channels,out_channels, kernel_size=kernel_size,padding=padding)
        self.layer2 = nn.Conv2d(out_channels,out_channels,kernel_size=kernel_size,padding=padding)
        self.last = last
        
    def forward(self, x):
        o1 = self.layer1(x)
        o1 = self.batchnorm1(o1)
        o1 = self.activation(o1)
        o2 = self.layer2(o1)
        o2 = self.batchnorm2(o2)
        if self.last:
            return o2
        else:
            return self.activation(o2)

class ShatzNet(nn.Module):
    
    def __init__(self,deep=False):
        super(ShatzNet, self).__init__()
        
        #WNet
        self.x00 = WNetNode(3,  64,  last=False)
        self.x10 = WNetNode(64, 128, last=False)
        self.x20 = WNetNode(128,256, last=False)
        self.x30 = WNetNode(256,512, last=False)
        self.x40 = WNetNode(512,1024,last=False)
        
        self.xm1 = WNetNode(128,  64, last=False)
        self.xm2 = WNetNode(256,  128,last=False)
        self.xm3 = WNetNode(512,  256,last=False)
        self.xm4 = WNetNode(1024, 512,last=False)
        
        self.x31 = WNetNode(1024,512,last=False)
        self.x22 = WNetNode(512, 256,last=False)
        self.x13 = WNetNode(256, 128,last=False)
        self.x04 = WNetNode(128, 64, last=False)
        
        self.up1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)
        self.up2 = nn.ConvTranspose2d(512,256, kernel_size=2,stride=2)
        self.up3 = nn.ConvTranspose2d(256,128, kernel_size=2,stride=2)
        self.up4 = nn.ConvTranspose2d(128,64,  kernel_size=2,stride=2)
        
        self.mp1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)
        self.mp2 = nn.ConvTranspose2d(512,256, kernel_size=2,stride=2)
        self.mp3 = nn.ConvTranspose2d(256,128, kernel_size=2,stride=2)
        self.mp4 = nn.ConvTranspose2d(128,64,  kernel_size=2,stride=2)
        
        self.wlst = WNetNode(64, 1,last=False)
        
        #UNet
        self.y00 = WNetNode(1,  64,  last=False)
        self.y10 = WNetNode(64, 128, last=False)
        self.y20 = WNetNode(128,256, last=False)
        self.y30 = WNetNode(256,512, last=False)
        self.y40 = WNetNode(512,1024,last=False)
        
        self.y31 = WNetNode(1024,512,last=False)
        self.y22 = WNetNode(512, 256,last=False)
        self.y13 = WNetNode(256, 128,last=False)
        self.y04 = WNetNode(128, 64, last=False)
        self.ulst = WNetNode(64,  1,  last=True)
        
        self.yup1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)
        self.yup2 = nn.ConvTranspose2d(512,256, kernel_size=2,stride=2)
        self.yup3 = nn.ConvTranspose2d(256,128, kernel_size=2,stride=2)
        self.yup4 = nn.ConvTranspose2d(128,64,  kernel_size=2,stride=2)
        
    def forward(self,x):
        #propagate through wnet
        o1  = self.x00(x)                       #o1 :512x512 x 64
        o1d = nn.MaxPool2d(2)(o1)               #o1d:256x256 x 64
        o2  = self.x10(o1d)                     #o2 :256x256 x 128
        o2d = nn.MaxPool2d(2)(o2)               #o2d:128x128 x 128
        o3  = self.x20(o2d)                     #o3 :128x128 x 256
        o3d = nn.MaxPool2d(2)(o3)               #o3d:64x64   x 256
        o4  = self.x30(o3d)                     #o4 :64x64   x 512
        o4d = nn.MaxPool2d(2)(o4)               #o4d:32x32   x 512
        
        b = self.x40(o4d)                       #b  :32x32   x 1024
        
        #intermediate layers
        mi1 = self.mp1(b)                       #mi1:64x64   x 512
        mo1 = self.xm4(torch.cat([mi1,o4],1))   #mo1:64x64   x 512
        mi2 = self.mp2(mo1)                     #mi2:128x128 x 256
        mo2 = self.xm3(torch.cat([mi2,o3],1))   #mo2:128x128 x 256
        mo2  = nn.Dropout(0.5)(mo2)
        mi3 = self.mp3(mo2)                     #mi3:256x256 x 128
        mo3 = self.xm2(torch.cat([mi3,o2],1))   #mo3:256x256 x 128
        mo3  = nn.Dropout(0.5)(mo3)
        mi4 = self.mp4(mo3)                     #mi4:512x512 x 64
        mo4 = self.xm1(torch.cat([mi4,o1],1))   #mo4:512x512 x 64
        
        o5u = self.up1(b)                       #o5u:64x64   x 512
        o6  = self.x31(torch.cat([o5u,mo1],1))  #o6 :64x64   x 512
        o6  = nn.Dropout(0.5)(o6)
        o6u = self.up2(o6)                      #o6u:128x128 x 256
        o7  = self.x22(torch.cat([o6u,mo2],1))  #o7 :128x128 x 256
        o7  = nn.Dropout(0.5)(o7)
        o7u = self.up3(o7)                      #o7u:256x256 x 128
        o8  = self.x13(torch.cat([o7u,mo3],1))  #o8: 256x256 x 128
        o8  = nn.Dropout(0.5)(o8)
        o8u = self.up4(o8)                      #o8u:512x512 x 64
        o9  = self.x04(torch.cat([o8u,mo4],1))  #o9: 512x512 x 64

        out_w = self.wlst(o9)                   #out_w:512x512 x 1
        
        #propagate through unet
        yo1  = self.y00(out_w)                    #yo1 :512x512 x 64
        yo1d = nn.MaxPool2d(2)(yo1)               #yo1d:256x256 x 64
        yo2  = self.y10(yo1d)                     #yo2 :256x256 x 128
        yo2d = nn.MaxPool2d(2)(yo2)               #yo2d:128x128 x 128
        yo3  = self.y20(yo2d)                     #yo3 :128x128 x 256
        yo3d = nn.MaxPool2d(2)(yo3)               #yo3d:64x64   x 256
        yo4  = self.y30(yo3d)                     #yo4 :64x64   x 512
        yo4d = nn.MaxPool2d(2)(yo4)               #yo4d:32x32   x 512
        
        yb = self.x40(yo4d)                        #yb  :32x32   x 1024
        
        yo5u = self.yup1(yb)                       #yo5u:64x64   x 512
        yo6  = self.y31(torch.cat([yo5u,yo4],1))   #yo6 :64x64   x 512
        yo6  = nn.Dropout(0.5)(yo6)
        yo6u = self.yup2(yo6)                      #yo6u:128x128 x 256
        yo7  = self.y22(torch.cat([yo6u,yo3],1))   #yo7 :128x128 x 256
        yo7  = nn.Dropout(0.5)(yo7)
        yo7u = self.yup3(yo7)                      #yo7u:256x256 x 128
        yo8  = self.y13(torch.cat([yo7u,yo2],1))   #yo8: 256x256 x 128
        yo8  = nn.Dropout(0.5)(yo8)
        yo8u = self.yup4(yo8)                      #yo8u:512x512 x 64
        yo9  = self.y04(torch.cat([yo8u,yo1],1))   #yo9: 512x512 x 64
        
        yout = self.ulst(yo9)                     #yout:512x512 x 1
        yout = nn.Sigmoid()(yout)                 #logit last layer
        return yout

def to_submission(model_output,image_id,median_filter=False):
    csv_arr = []
    if median_filter:
        model_output = scipy.ndimage.median_filter(model_output, 8)
    for i in range(0,model_output.shape[0],16):
        for j in range(0,model_output.shape[1],16):
            prediction = 0
            patch = model_output[j:j+16, i:i+16]
            if np.mean(patch) > 0.25:
                prediction = 1
            csv_arr.append(["{:03d}_{}_{}".format(image_id,i,j),prediction])
    return csv_arr

def train_no_valid(net,train_data,train_labels,epochs,loss,eta,batch_size,weight_decay,eps=1e-5):
    optimizer = torch.optim.Adam(net.parameters(),lr=eta,weight_decay=weight_decay)
    running_loss = 0
    print(f'Starting training with eta={eta} for {epochs} epochs, with batch size {batch_size}')

    #training
    for epoch in tqdm(range(epochs)):
        perm = np.random.permutation(train_data.shape[0])
        train_data = train_data[perm]
        train_labels = train_labels[perm]
        k=0
        print(f'Training for Epoch {epoch+1} starting:')
        net.train()
        loss_train = 0
        for img,lab in batch(train_data,train_labels,batch_size):
            if use_cuda:
                inp = torch.from_numpy(img).float().cuda()
                lab = torch.from_numpy(lab).float().cuda()
            else:
                inp = torch.from_numpy(img).float()
                lab = torch.from_numpy(lab).float()
                
            optimizer.zero_grad()
            inference = net(inp)
            loss_v = loss(inference,lab)
            loss_v.backward()
            optimizer.step()
            
            loss_train += loss_v
            running_loss += loss_v
            k+=1
            if k % 10 == 0:
                print(f'Epoch {epoch+1}, avg running_loss: {running_loss/10}')
                running_loss=0
                k = 0
        print(f'Training loss for {epoch+1}:{loss_train}')
        
                    
        torch.save({
                'epoch': epoch,
                'model_state_dict': net.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': loss,
                }, MODEL_PATH)



# Load model from memory or initialize new model depending on the params
if LOAD_MODEL:
    net = ShatzNet()
    net.load_state_dict(torch.load(MODEL_PATH)['model_state_dict'])
else:
    net = ShatzNet()
if use_cuda:
    net.cuda()

# Data Generation
#read data
if LOAD_IMG:
    print('Reading training set from memory...')
    images = np.zeros(shape=(100,3,320,320))
    labels = np.empty(shape=(100,1,320,320))
    for i in tqdm(range(1,101)):
        image = np.array(Image.open(f'{TRAIN_PATH}/images/satImage_{str(i).zfill(3)}.png')).astype(np.float)/255
        label = np.array(Image.open(f'{TRAIN_PATH}/groundtruth/satImage_{str(i).zfill(3)}.png')).astype(np.float)/255
        image = resize(image,(320,320))
        label = resize(label,(320,320))
        image = np.expand_dims(np.transpose(image,(2,0,1)),0)
        label = np.expand_dims(label,0)
        images[i-1,:,:,:] = image
        labels[i-1,:,:,:] = label

if GENERATE_DATA:
    print('Generating data. This might take a while (~1 hour on most machines)')
    np.random.seed(69)
    
    rotations = [0,30,60,90]
    no_rotations = 4
    
    translation_range = (-8,8)
    no_translations = 2
    
    brightness = [0.8,1]
    no_brightness = 2
    
    zooms = [0.5,0.75,1,1.50]
    no_zooms = 4

    augmented_images = np.zeros(shape=(len(images)*no_rotations*no_translations*no_brightness*no_zooms,3,320,320))
    augmented_labels = np.zeros(shape=(len(labels)*no_rotations*no_translations*no_brightness*no_zooms,1,320,320))
    k=0
    for i in tqdm(range(images.shape[0])):
        for R in rotations:
            for T in range(no_translations):
                for B in range(no_brightness):
                    for zoom in zooms:
                        Ro_r = R
                        Tx_r = np.random.uniform(low=translation_range[0], high=translation_range[1], size=1)[0]
                        Ty_r = np.random.uniform(low=translation_range[0], high=translation_range[1], size=1)[0]
                        Br_r = brightness[B]
                        
                        px = np.abs(int(320*(1-zoom))//2)
                        py = np.abs(int(320*(1-zoom))//2)
                        if zoom <= 1:
                            zoom_img = np.pad(scipy.ndimage.zoom(np.transpose(images[i],(1,2,0)),(zoom,zoom,1),mode='reflect'),((px,px),(py,py),(0,0)),mode='reflect')
                        else:
                            zoom_img = scipy.ndimage.zoom(np.transpose(images[i],(1,2,0)),(zoom,zoom,1),mode='reflect')
                            zoom_img = zoom_img[px:zoom_img.shape[0]-px,py:zoom_img.shape[1]-py,:]
                        rot_img = scipy.ndimage.rotate(zoom_img, Ro_r, reshape=False, mode='reflect')
                        trn_img = scipy.ndimage.shift(rot_img,(Tx_r,Ty_r,0), mode='reflect')
                        
                        if zoom <= 1:
                            zoom_lab = np.pad(scipy.ndimage.zoom(np.transpose(labels[i],(1,2,0)),(zoom,zoom,1),mode='reflect'),((px,px),(py,py),(0,0)),mode='reflect')
                        else:
                            zoom_lab = scipy.ndimage.zoom(np.transpose(labels[i],(1,2,0)),(zoom,zoom,1),mode='reflect')
                            zoom_lab = zoom_lab[px:zoom_lab.shape[0]-px,py:zoom_lab.shape[1]-py]
                        rot_lab = scipy.ndimage.rotate(zoom_lab, Ro_r, reshape=False, mode='reflect')
                        trn_lab = scipy.ndimage.shift(rot_lab,(Tx_r,Ty_r,0), mode='reflect')
                        
                        fin_img = np.transpose(Br_r*trn_img,(2,0,1))
                        fin_lab = np.transpose(trn_lab,(2,0,1)) > 0.3
                        augmented_images[k,:,:,:] = np.clip(fin_img,0,1)
                        augmented_labels[k,:,:,:] = fin_lab
                        k+=1
                        
    perm = np.random.permutation(augmented_images.shape[0])
    augmented_images = augmented_images[perm]
    augmented_labels = augmented_labels[perm]
    
    print("Saving data on disk. Takes around ~20GB space...")
    np.save(f'{DATA_PATH}augmented_images_train_best',augmented_images)
    np.save(f'{DATA_PATH}augmented_labels_train_best',augmented_labels)

if LOAD_DATA:
    print('Loading generated data from memory...')
    augmented_images_train = np.load(f'{DATA_PATH}augmented_images_train_best.npy')
    augmented_labels_train = np.load(f'{DATA_PATH}augmented_labels_train_best.npy')

if TRAIN:
    print('Training information: 60 epochs, first 30 with eta=3e-4, then 15 with eta=3e-5, then 15 with eta-3e-6. May take a while. You may run out of CUDA memory. You have been warned.')
    train_no_valid(net,train_data=augmented_images_train,train_labels=augmented_labels_train,epochs=30,loss=nn.BCELoss(),eta=3e-4,batch_size=8,weight_decay=1e-6)
    train_no_valid(net,train_data=augmented_images_train,train_labels=augmented_labels_train,epochs=15,loss=nn.BCELoss(),eta=3e-5,batch_size=8,weight_decay=1e-7)
    train_no_valid(net,train_data=augmented_images_train,train_labels=augmented_labels_train,epochs=15,loss=nn.BCELoss(),eta=3e-6,batch_size=8,weight_decay=1e-8)

outputs = []
for img_idx in tqdm(range(1,51)):
    net.eval()
    test_img = resize(np.array(Image.open(f'{TEST_PATH}/test_{img_idx}/test_{img_idx}.png')).astype('float32') / 255, (320,320))
    test_img_n = np.expand_dims(test_img,0)
    test_img_n = torch.from_numpy(np.transpose(test_img_n,(0,3,1,2))).float()
    if use_cuda:
        out = resize(net(test_img_n.cuda()).detach().cpu().numpy()[0,0,:,:], (608,608))
    else:
        out = resize(net(test_img_n).detach().numpy()[0,0,:,:], (608,608))
    sub_output = to_submission(out,img_idx)
    outputs.append(sub_output)

outputs_np = np.concatenate(outputs,axis=0)
outputs_np = np.concatenate(([['id','prediction']],outputs_np), axis=0)
np.savetxt("submit.csv", outputs_np, delimiter=",", fmt = '%s')